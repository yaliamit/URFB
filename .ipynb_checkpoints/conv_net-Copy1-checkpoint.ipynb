{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "\n",
    "# Convert classes to indicator vectors\n",
    "def one_hot(values,n_values=10):\n",
    "    n_v = np.maximum(n_values,np.max(values) + 1)\n",
    "    oh=np.eye(n_v)[values]\n",
    "    return oh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Mnist data and split into train validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "\n",
    "    data=np.float64(np.load('/Users/amit/Box Sync/tex/courses/LSDA/dataset/MNIST.npy'))\n",
    "    labels=np.float32(np.load('/project/cmsc25025/mnist/MNIST_labels.npy'))\n",
    "    print(data.shape)\n",
    "    data=np.float32(data)/255.\n",
    "    train_dat=data[0:50000]\n",
    "    train_labels=one_hot(np.int32(labels[0:50000]))\n",
    "    val_dat=data[50000:60000]\n",
    "    val_labels=one_hot(np.int32(labels[50000:60000]))\n",
    "    test_dat=data[60000:70000]\n",
    "    test_labels=one_hot(np.int32(labels[60000:70000]))\n",
    "    \n",
    "    return (train_dat, train_labels), (val_dat, val_labels), (test_dat, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get CIFAR10 data and split into train validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar():\n",
    "    tr=np.float32(np.load('../../mnist/CIFAR_10.npy'))\n",
    "    tr_lb=np.int32(np.load('/project/cmsc25025/mnist/CIFAR_labels.npy'))\n",
    "    tr=tr.reshape((-1,np.prod(np.array(tr.shape)[1:4])))\n",
    "    train_data=tr[0:45000]/255.\n",
    "    train_labels=one_hot(tr_lb[0:45000])\n",
    "    val_data=tr[45000:]/255.\n",
    "    val_labels=one_hot(tr_lb[45000:])\n",
    "    test_data=np.float32(np.load('/project/cmsc25025/mnist/CIFAR_10_test.npy'))\n",
    "    test_data=test_data.reshape((-1,np.prod(np.array(test_data.shape)[1:4])))\n",
    "    test_data=test_data/255.\n",
    "    test_labels=one_hot(np.int32(np.load('/project/cmsc25025/mnist/CIFAR_labels_test.npy')))\n",
    "    return (train_data, train_labels), (val_data, val_labels), (test_data, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get transformed Mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_trans():\n",
    "    test_trans_dat=np.float32(np.load('/project/cmsc25025/mnist/MNIST_TEST_TRANS.npy'))\n",
    "    test_labels=one_hot(np.int32(np.float32(np.load('/project/cmsc25025/mnist/MNIST_labels.npy'))))\n",
    "    return (test_trans_dat, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution layer with relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_layer(input,filter_size=[3,3],num_features=[1]):\n",
    "\n",
    "    # Get number of input features from input and add to shape of new layer\n",
    "    shape=filter_size+[input.get_shape().as_list()[-1],num_features]\n",
    "    W = tf.get_variable('W',shape=shape) # Default initialization is Glorot (the one explained in the slides)\n",
    "    b = tf.get_variable('b',shape=[num_features],initializer=tf.zeros_initializer) \n",
    "    conv = tf.nn.conv2d(input, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu = tf.nn.relu(conv + b)\n",
    "    return(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layer(input,num_features):\n",
    "    # Make sure input is flattened.\n",
    "    flat_dim=np.int32(np.array(input.get_shape().as_list())[1:].prod())\n",
    "    input_flattened = tf.reshape(input, shape=[-1,flat_dim])\n",
    "    shape=[flat_dim,num_features]\n",
    "    W_fc = tf.get_variable('W',shape=shape) \n",
    "    b_fc = tf.get_variable('b',shape=[num_features],initializer=tf.zeros_initializer)\n",
    "    fc = tf.matmul(input_flattened, W_fc) + b_fc\n",
    "    return(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "def create_network():\n",
    "    pool_ksize=[1,2,2,1]\n",
    "    pool_strides=[1,2,2,1]\n",
    "    # The network:\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "            # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "            relu1 = conv_relu_layer(x_image, filter_size=[5, 5],num_features=32)\n",
    "            pool1 = tf.nn.max_pool(relu1, ksize=pool_ksize, strides=pool_strides, padding='SAME')\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "            # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "            relu2 = conv_relu_layer(pool1, filter_size=[5, 5],num_features=64)\n",
    "            pool2 = tf.nn.max_pool(relu2, ksize=pool_ksize, strides=pool_strides, padding='SAME')\n",
    "    with tf.variable_scope('dropout2'):\n",
    "            drop2=tf.nn.dropout(pool2,keep_prob)\n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "            fc1 = fully_connected_layer(drop2, num_features=256)\n",
    "            fc1r=tf.nn.relu(fc1)\n",
    "   \n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "            fc2 = fully_connected_layer(fc1r, num_features=10)\n",
    "\n",
    "    # Names (OUT,LOSS, ACC) below added to make it easier to use this tensor when restoring model\n",
    "    fc2 = tf.identity(fc2, name=\"OUT\")\n",
    "    # The loss computation\n",
    "    with tf.variable_scope('cross_entropy_loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=fc2),name=\"LOSS\")\n",
    "\n",
    "    # Accuracy computation\n",
    "    with tf.variable_scope('helpers'):\n",
    "        correct_prediction = tf.equal(tf.argmax(fc2, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"ACC\")\n",
    "    # We return the final functions (they contain all the information about the graph of the network)\n",
    "    return cross_entropy, accuracy, fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get loss and accuracy on a data set with output from final layer fc2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get loss and accuracy from only one run of the feature extraction network\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def get_stats(data,labels):\n",
    "    t1=time.time()\n",
    "    lo=0.\n",
    "    acc=0.\n",
    "    delta=1000\n",
    "    rr=np.arange(0,data.shape[0],delta)\n",
    "    for i in rr:\n",
    "        fc2_out=fc2.eval(feed_dict={x: data[i:i+delta], y_:labels[i:i+delta]})\n",
    "        log_sf=logsumexp(fc2_out,axis=1).reshape((fc2_out.shape[0],1))-fc2_out\n",
    "        lo+=np.mean(np.sum(labels[i:i+delta]*log_sf, axis=1))\n",
    "        acc += np.mean(np.equal(np.argmax(fc2_out, axis=1),np.argmax(labels[i:i+delta], axis=1)))\n",
    "    acc=acc/np.float32(len(rr))\n",
    "    lo=lo/np.float32(len(rr))\n",
    "    print('get stats time',time.time()-t1)\n",
    "    # We return the final functions (they contain all the information about the graph of the network)\n",
    "    return lo, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the iterations of one epoch\n",
    "def run_epoch(train,val,ii,batch_size,train_step_new):\n",
    "        t1=time.time()\n",
    "        # Randomly shuffle the training data\n",
    "        np.random.shuffle(ii)\n",
    "        tr=train[0][ii]\n",
    "        y=train[1][ii]\n",
    "        lo=0.\n",
    "        acc=0.\n",
    "        # Run disjoint batches on shuffled data\n",
    "        for j in np.arange(0,len(y),batch_size):\n",
    "            if (np.mod(j,5000)==0):\n",
    "                print('Batch',j/batch_size)\n",
    "            batch=(tr[j:j+batch_size],y[j:j+batch_size])\n",
    "            train_step_new.run(feed_dict={x: batch[0], y_: batch[1], lr_: step_size,keep_prob_:keep_prob})\n",
    "        print('Epoch time',time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data and run the training. Save the model and test at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_set):\n",
    "    if (data_set==\"cifar\"):\n",
    "        return(get_cifar())\n",
    "    elif (data_set==\"mnist\"):\n",
    "        return(get_mnist())\n",
    "    elif (data_set==\"mnist_transform\"):\n",
    "        return(get_mnist_trans())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-db2e7185f21f>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "conv1/W:0 [5, 5, 3, 32] 0.047882065\n",
      "conv1/b:0 [32] 0.0\n",
      "conv2/W:0 [5, 5, 32, 64] 0.028840385\n",
      "conv2/b:0 [64] 0.0\n",
      "fc1/W:0 [4096, 256] 0.021425525\n",
      "fc1/b:0 [256] 0.0\n",
      "fc2/W:0 [256, 10] 0.08606616\n",
      "fc2/b:0 [10] 0.0\n",
      "Batch 0.0\n",
      "Batch 10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8a37bbdb11b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Run epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4443338ecd05>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(train, val, ii, batch_size, train_step_new)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtrain_step_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2278\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m     \"\"\"\n\u001b[0;32m-> 2280\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5049\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5050\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5051\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "\n",
    "import time\n",
    "batch_size=500\n",
    "step_size=.001\n",
    "num_epochs=4\n",
    "num_train=10000\n",
    "minimizer=\"Adam\"\n",
    "data_set=\"cifar\"\n",
    "model_name=\"model\"\n",
    "keep_prob=.5\n",
    "dim=28\n",
    "nchannels=1\n",
    "if (data_set==\"cifar\"):\n",
    "    dim=32\n",
    "    nchannels=3\n",
    "\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, dim*dim*nchannels],name=\"x\")\n",
    "x_image = tf.reshape(x, [-1, dim, dim, nchannels])\n",
    "# Dimensions of x_image: [Batch size, Column size, Row size, Number of incoming channels]\n",
    "# The number of incoming channels, for example, will be 3 if the image is color: RGB (red, green, blue)\n",
    "# We will slide filter over this 2d picture with conv2d function.\n",
    "y_ = tf.placeholder(tf.float32, shape=[None,10],name=\"y\")\n",
    "# Allows you to control the time step during the iterations\n",
    "lr_ = tf.placeholder(tf.float32, shape=[],name=\"learning_rate\")\n",
    "keep_prob_=tf.placeholder(tf.float32, shape=[],name=\"keep_prob\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train,val,test=get_data(data_set=data_set)\n",
    "    # Create the network architecture with the above placeholdes as the inputs.\n",
    "    cross_entropy, accuracy, fc2 =create_network()\n",
    "\n",
    "    # Define the miminization method\n",
    "    if (minimizer==\"Adam\"):\n",
    "        train_step=tf.train.AdamOptimizer(learning_rate=lr_).minimize(cross_entropy)\n",
    "    elif (minimizer==\"SGD\"):\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate=lr_).minimize(cross_entropy)\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Show trainable variables\n",
    "    for v in tf.trainable_variables():\n",
    "        print(v.name,v.get_shape().as_list(),np.std(v.eval()))\n",
    "    ii=np.arange(0,num_train,1) #len(train_data),1)\n",
    "    # Run epochs\n",
    "    for i in range(num_epochs):  # number of epochs\n",
    "        run_epoch(train,val,ii,batch_size,train_step)\n",
    "        if (np.mod(i,2)==0):\n",
    "            lo,ac = get_stats(train[0][0:num_train],train[1][0:num_train])\n",
    "            print('Epoch',i,'Train loss, accuracy',lo,ac)\n",
    "            vlo,vac = get_stats(val[0],val[1])\n",
    "            print('EPoch',i,'Validation loss, accuracy',vlo,vac)\n",
    "            # Test set accuracy\n",
    " \n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: test[0], y_:test[1]}))\n",
    "    \n",
    "    # Save model\n",
    "    tf.add_to_collection(\"optimizer\", train_step)\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"tmp/\"+model_name)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload the model that was saved and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading an existing model.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "batch_size=500\n",
    "step_size=.001\n",
    "num_epochs=4\n",
    "num_train=10000\n",
    "data_set=\"cifar\"\n",
    "model_name=\"model\"\n",
    "Train=True\n",
    "dim=28\n",
    "nchannels=1\n",
    "if (data_set==\"cifar\"):\n",
    "    dim=32\n",
    "    nchannels=3\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # Get data\n",
    "    train, val, test=get_data(data_set=data_set)\n",
    "    # Load model info\n",
    "    saver = tf.train.import_meta_graph('tmp/'+model_name+'.meta')\n",
    "    saver.restore(sess,'tmp/'+model_name) \n",
    "    graph = tf.get_default_graph()\n",
    "    # Setup the placeholders from the stored model.\n",
    "    x = graph.get_tensor_by_name('x:0')\n",
    "    y_= graph.get_tensor_by_name('y:0')\n",
    "    lr_ = graph.get_tensor_by_name('learning_rate:0')\n",
    "    keep_prob_ = graph.get_tensor_by_name('keep_prob:0')\n",
    "    accuracy=graph.get_tensor_by_name('helpers/ACC:0')\n",
    "    cross_entropy=graph.get_tensor_by_name('cross_entropy_loss/LOSS:0')\n",
    "    fc2=graph.get_tensor_by_name('OUT:0')\n",
    "    # Get the minimization operation from the stored model\n",
    "    if (Train):\n",
    "        train_step_new = tf.get_collection(\"optimizer\")[0]\n",
    "        # Confirm training accuracy of current model before additional training\n",
    "        acc=accuracy.eval(feed_dict={x: train[0][0:num_train], y_:train[1][0:num_train]})\n",
    "        print('train acc',acc)\n",
    "\n",
    "        ii=np.arange(0,num_train,1) \n",
    "        for i in range(num_epochs):  # Run epochs\n",
    "            run_epoch(train,val,ii,batch_size,train_step_new)\n",
    "            if (np.mod(i,2)==0):\n",
    "                lo,ac = get_stats(train[0][0:num_train],train[1][0:num_train])\n",
    "                print('Epoch',i,'Train loss, accuracy',lo,ac)\n",
    "                vlo,vac = get_stats(val[0],val[1])\n",
    "                print('EPoch',i,'Validation loss, accuracy',vlo,vac)\n",
    "    # Test set accuracy\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: test[0], y_:test[1]}))\n",
    "    \n",
    "    tf.add_to_collection(\"optimizer\", train_step)\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"tmp/\"+model_name)\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
